{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset and preprocess\n",
    "df = pd.read_csv(\"preprocessed_dataset.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                     % Iron Feed  % Silica Feed  Starch Flow  Amina Flow  \\\ndate                                                                       \n2017-03-10 01:03:00        56.39          13.53      2082.27     391.733   \n2017-03-10 01:03:20        55.20          16.98      3019.53     557.434   \n2017-03-10 01:03:40        55.20          16.98      3024.41     563.965   \n2017-03-10 01:04:00        55.20          16.98      3043.46     568.054   \n2017-03-10 01:04:20        55.20          16.98      3047.36     568.665   \n\n                     Ore Pulp Flow  Ore Pulp pH  Ore Pulp Density  \\\ndate                                                                \n2017-03-10 01:03:00        403.738      9.70067           1.74925   \n2017-03-10 01:03:20        395.713     10.06640           1.74000   \n2017-03-10 01:03:40        397.383     10.06720           1.74000   \n2017-03-10 01:04:00        399.668     10.06800           1.74000   \n2017-03-10 01:04:20        397.939     10.06890           1.74000   \n\n                     Flotation Column 01 Air Flow  \\\ndate                                                \n2017-03-10 01:03:00                       250.509   \n2017-03-10 01:03:20                       249.214   \n2017-03-10 01:03:40                       249.719   \n2017-03-10 01:04:00                       249.741   \n2017-03-10 01:04:20                       249.917   \n\n                     Flotation Column 02 Air Flow  \\\ndate                                                \n2017-03-10 01:03:00                       251.163   \n2017-03-10 01:03:20                       253.235   \n2017-03-10 01:03:40                       250.532   \n2017-03-10 01:04:00                       247.874   \n2017-03-10 01:04:20                       254.487   \n\n                     Flotation Column 03 Air Flow  ...  \\\ndate                                               ...   \n2017-03-10 01:03:00                       250.610  ...   \n2017-03-10 01:03:20                       250.576  ...   \n2017-03-10 01:03:40                       250.862  ...   \n2017-03-10 01:04:00                       250.313  ...   \n2017-03-10 01:04:20                       250.049  ...   \n\n                     Flotation Column 06 Air Flow  \\\ndate                                                \n2017-03-10 01:03:00                       250.389   \n2017-03-10 01:03:20                       250.225   \n2017-03-10 01:03:40                       250.137   \n2017-03-10 01:04:00                       251.345   \n2017-03-10 01:04:20                       250.422   \n\n                     Flotation Column 07 Air Flow  Flotation Column 01 Level  \\\ndate                                                                           \n2017-03-10 01:03:00                       249.430                    606.023   \n2017-03-10 01:03:20                       250.884                    457.396   \n2017-03-10 01:03:40                       248.994                    451.891   \n2017-03-10 01:04:00                       248.071                    451.240   \n2017-03-10 01:04:20                       251.147                    452.441   \n\n                     Flotation Column 02 Level  Flotation Column 03 Level  \\\ndate                                                                        \n2017-03-10 01:03:00                    596.422                    604.776   \n2017-03-10 01:03:20                    432.962                    424.954   \n2017-03-10 01:03:40                    429.560                    432.939   \n2017-03-10 01:04:00                    468.927                    434.610   \n2017-03-10 01:04:20                    458.165                    442.865   \n\n                     Flotation Column 04 Level  Flotation Column 05 Level  \\\ndate                                                                        \n2017-03-10 01:03:00                    437.700                    430.352   \n2017-03-10 01:03:20                    443.558                    502.255   \n2017-03-10 01:03:40                    448.086                    496.363   \n2017-03-10 01:04:00                    449.688                    484.411   \n2017-03-10 01:04:20                    446.210                    471.411   \n\n                     Flotation Column 06 Level  Flotation Column 07 Level  \\\ndate                                                                        \n2017-03-10 01:03:00                    433.920                    443.036   \n2017-03-10 01:03:20                    446.370                    523.344   \n2017-03-10 01:03:40                    445.922                    498.075   \n2017-03-10 01:04:00                    447.826                    458.567   \n2017-03-10 01:04:20                    437.690                    427.669   \n\n                     % Silica Concentrate  \ndate                                       \n2017-03-10 01:03:00                  1.40  \n2017-03-10 01:03:20                  1.31  \n2017-03-10 01:03:40                  1.31  \n2017-03-10 01:04:00                  1.31  \n2017-03-10 01:04:20                  1.31  \n\n[5 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>% Iron Feed</th>\n      <th>% Silica Feed</th>\n      <th>Starch Flow</th>\n      <th>Amina Flow</th>\n      <th>Ore Pulp Flow</th>\n      <th>Ore Pulp pH</th>\n      <th>Ore Pulp Density</th>\n      <th>Flotation Column 01 Air Flow</th>\n      <th>Flotation Column 02 Air Flow</th>\n      <th>Flotation Column 03 Air Flow</th>\n      <th>...</th>\n      <th>Flotation Column 06 Air Flow</th>\n      <th>Flotation Column 07 Air Flow</th>\n      <th>Flotation Column 01 Level</th>\n      <th>Flotation Column 02 Level</th>\n      <th>Flotation Column 03 Level</th>\n      <th>Flotation Column 04 Level</th>\n      <th>Flotation Column 05 Level</th>\n      <th>Flotation Column 06 Level</th>\n      <th>Flotation Column 07 Level</th>\n      <th>% Silica Concentrate</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-03-10 01:03:00</th>\n      <td>56.39</td>\n      <td>13.53</td>\n      <td>2082.27</td>\n      <td>391.733</td>\n      <td>403.738</td>\n      <td>9.70067</td>\n      <td>1.74925</td>\n      <td>250.509</td>\n      <td>251.163</td>\n      <td>250.610</td>\n      <td>...</td>\n      <td>250.389</td>\n      <td>249.430</td>\n      <td>606.023</td>\n      <td>596.422</td>\n      <td>604.776</td>\n      <td>437.700</td>\n      <td>430.352</td>\n      <td>433.920</td>\n      <td>443.036</td>\n      <td>1.40</td>\n    </tr>\n    <tr>\n      <th>2017-03-10 01:03:20</th>\n      <td>55.20</td>\n      <td>16.98</td>\n      <td>3019.53</td>\n      <td>557.434</td>\n      <td>395.713</td>\n      <td>10.06640</td>\n      <td>1.74000</td>\n      <td>249.214</td>\n      <td>253.235</td>\n      <td>250.576</td>\n      <td>...</td>\n      <td>250.225</td>\n      <td>250.884</td>\n      <td>457.396</td>\n      <td>432.962</td>\n      <td>424.954</td>\n      <td>443.558</td>\n      <td>502.255</td>\n      <td>446.370</td>\n      <td>523.344</td>\n      <td>1.31</td>\n    </tr>\n    <tr>\n      <th>2017-03-10 01:03:40</th>\n      <td>55.20</td>\n      <td>16.98</td>\n      <td>3024.41</td>\n      <td>563.965</td>\n      <td>397.383</td>\n      <td>10.06720</td>\n      <td>1.74000</td>\n      <td>249.719</td>\n      <td>250.532</td>\n      <td>250.862</td>\n      <td>...</td>\n      <td>250.137</td>\n      <td>248.994</td>\n      <td>451.891</td>\n      <td>429.560</td>\n      <td>432.939</td>\n      <td>448.086</td>\n      <td>496.363</td>\n      <td>445.922</td>\n      <td>498.075</td>\n      <td>1.31</td>\n    </tr>\n    <tr>\n      <th>2017-03-10 01:04:00</th>\n      <td>55.20</td>\n      <td>16.98</td>\n      <td>3043.46</td>\n      <td>568.054</td>\n      <td>399.668</td>\n      <td>10.06800</td>\n      <td>1.74000</td>\n      <td>249.741</td>\n      <td>247.874</td>\n      <td>250.313</td>\n      <td>...</td>\n      <td>251.345</td>\n      <td>248.071</td>\n      <td>451.240</td>\n      <td>468.927</td>\n      <td>434.610</td>\n      <td>449.688</td>\n      <td>484.411</td>\n      <td>447.826</td>\n      <td>458.567</td>\n      <td>1.31</td>\n    </tr>\n    <tr>\n      <th>2017-03-10 01:04:20</th>\n      <td>55.20</td>\n      <td>16.98</td>\n      <td>3047.36</td>\n      <td>568.665</td>\n      <td>397.939</td>\n      <td>10.06890</td>\n      <td>1.74000</td>\n      <td>249.917</td>\n      <td>254.487</td>\n      <td>250.049</td>\n      <td>...</td>\n      <td>250.422</td>\n      <td>251.147</td>\n      <td>452.441</td>\n      <td>458.165</td>\n      <td>442.865</td>\n      <td>446.210</td>\n      <td>471.411</td>\n      <td>437.690</td>\n      <td>427.669</td>\n      <td>1.31</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 22 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Useful variables selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO\n"
     ]
    }
   ],
   "source": [
    "print(\"TODO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### PLS loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "illegal value in 4th argument of internal gesdd",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 93\u001B[0m\n\u001B[0;32m     91\u001B[0m win_X \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(training_data[:, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     92\u001B[0m win_y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(np\u001B[38;5;241m.\u001B[39marray(training_data[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 93\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwin_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwin_y\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;66;03m# Predict the target values\u001B[39;00m\n\u001B[0;32m     96\u001B[0m y_train_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(win_X)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PLS\\Lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:643\u001B[0m, in \u001B[0;36mPLSRegression.fit\u001B[1;34m(self, X, Y)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, Y):\n\u001B[0;32m    626\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Fit model to data.\u001B[39;00m\n\u001B[0;32m    627\u001B[0m \n\u001B[0;32m    628\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    641\u001B[0m \u001B[38;5;124;03m        Fitted model.\u001B[39;00m\n\u001B[0;32m    642\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 643\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    644\u001B[0m     \u001B[38;5;66;03m# expose the fitted attributes `x_scores_` and `y_scores_`\u001B[39;00m\n\u001B[0;32m    645\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_scores_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_x_scores\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PLS\\Lib\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PLS\\Lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:353\u001B[0m, in \u001B[0;36m_PLS.fit\u001B[1;34m(self, X, Y)\u001B[0m\n\u001B[0;32m    342\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_loadings_[:, k] \u001B[38;5;241m=\u001B[39m y_loadings\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# X was approximated as Xi . Gamma.T + X_(R+1)\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# Xi . Gamma.T is a sum of n_components rank-1 matrices. X_(R+1) is\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# whatever is left to fully reconstruct X, and can be 0 if X is of rank\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    349\u001B[0m \n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# Compute transformation matrices (rotations_). See User Guide.\u001B[39;00m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_rotations_ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(\n\u001B[0;32m    352\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_weights_,\n\u001B[1;32m--> 353\u001B[0m     \u001B[43mpinv2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_loadings_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_weights_\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m,\n\u001B[0;32m    354\u001B[0m )\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_rotations_ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(\n\u001B[0;32m    356\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_weights_,\n\u001B[0;32m    357\u001B[0m     pinv2(np\u001B[38;5;241m.\u001B[39mdot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_loadings_\u001B[38;5;241m.\u001B[39mT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_weights_), check_finite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m    358\u001B[0m )\n\u001B[0;32m    359\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoef_ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_rotations_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_loadings_\u001B[38;5;241m.\u001B[39mT)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PLS\\Lib\\site-packages\\scipy\\linalg\\_basic.py:1434\u001B[0m, in \u001B[0;36mpinv\u001B[1;34m(a, atol, rtol, return_rank, check_finite, cond, rcond)\u001B[0m\n\u001B[0;32m   1321\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;124;03mCompute the (Moore-Penrose) pseudo-inverse of a matrix.\u001B[39;00m\n\u001B[0;32m   1323\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1431\u001B[0m \n\u001B[0;32m   1432\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1433\u001B[0m a \u001B[38;5;241m=\u001B[39m _asarray_validated(a, check_finite\u001B[38;5;241m=\u001B[39mcheck_finite)\n\u001B[1;32m-> 1434\u001B[0m u, s, vh \u001B[38;5;241m=\u001B[39m \u001B[43m_decomp_svd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msvd\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_matrices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1435\u001B[0m t \u001B[38;5;241m=\u001B[39m u\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mchar\u001B[38;5;241m.\u001B[39mlower()\n\u001B[0;32m   1436\u001B[0m maxS \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(s)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PLS\\Lib\\site-packages\\scipy\\linalg\\_decomp_svd.py:133\u001B[0m, in \u001B[0;36msvd\u001B[1;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001B[0m\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LinAlgError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSVD did not converge\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124millegal value in \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124mth argument of internal gesdd\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    134\u001B[0m                      \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m-\u001B[39minfo)\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compute_uv:\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m u, s, v\n",
      "\u001B[1;31mValueError\u001B[0m: illegal value in 4th argument of internal gesdd"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from model import Model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reduce the amount of observations (with averaging)\n",
    "df = df.resample(\"T\").mean()\n",
    "\n",
    "included_rows1 = [df.index.get_loc(datetime.datetime(2017,4,23,19,0,0)),\n",
    "                    df.index.get_loc(datetime.datetime(2017,5,13,0,59,00))]\n",
    "included_rows2 = [df.index.get_loc(datetime.datetime(2017,6,15,1,0,0)),\n",
    "                    df.index.get_loc(datetime.datetime(2017,7,24,0,59,00))]\n",
    "included_rows3 = [df.index.get_loc(datetime.datetime(2017,8,15,1,0,0)),\n",
    "                    df.index.get_loc(datetime.datetime(2017,9,10,0,0,0))]\n",
    "\n",
    "df_array = df.to_numpy()\n",
    "\n",
    "models = []\n",
    "\n",
    "# Find best window_size and num_of_components\n",
    "for window_size in [60]:  # [10, 20, 30, ...]\n",
    "    for num_of_components in [7]:  # range(1, len(df.columns)):\n",
    "        # Division of data into training, validation and test sets for the normal functioning times:\n",
    "        # about 70% for training, 15% for validation, 15% for testing\n",
    "        test_size = round(0.15*window_size)\n",
    "        test_start = '2017-04-23 00:00:00' # example start of test partition\n",
    "        test_start = datetime.datetime.strptime(test_start, '%Y-%m-%d %H:%M:%S')\n",
    "        flag = 0\n",
    "\n",
    "        model = PLSRegression(n_components=num_of_components)\n",
    "\n",
    "        row_number = df.index.get_loc(test_start)\n",
    "        scalers = []\n",
    "        train_mse_scores, train_q2_scores, train_r2_scores = [], [], []\n",
    "        valid_mse_scores, valid_q2_scores, valid_r2_scores = [], [], []\n",
    "        # Iterate through the data\n",
    "        while flag == 0:\n",
    "            validation_data, training_data = None, None\n",
    "            test_y_end = row_number+test_size+1\n",
    "            training_X_start = row_number - round(0.85*window_size)\n",
    "            # Stay within included rows\n",
    "            if ((test_y_end <= included_rows1[1] and training_X_start >= included_rows1[0])\n",
    "            or (test_y_end <= included_rows2[1] and training_X_start >= included_rows2[0])\n",
    "            or (test_y_end <= included_rows3[1] and training_X_start >= included_rows3[0])):\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "\n",
    "                # Validation set\n",
    "                validation_X = df_array[row_number-test_size:row_number,:-1]\n",
    "                validation_y = df_array[row_number-test_size+1:row_number+1,-1]\n",
    "\n",
    "                # Training set\n",
    "                training_X = df_array[training_X_start:row_number-test_size,:-1]\n",
    "                training_y = df_array[training_X_start+1:row_number-test_size+1,-1]\n",
    "\n",
    "                # TODO\n",
    "                # We should scale only X, correct?\n",
    "                # Compute the mean and std with training data\n",
    "                scaler.fit(training_X)\n",
    "                # Perform standardization\n",
    "                training_X = scaler.transform(training_X)\n",
    "                validation_X = scaler.transform(validation_X)\n",
    "\n",
    "                training_y = training_y\n",
    "                validation_y = validation_y\n",
    "\n",
    "                # Lagged predictor variables and the response variable in one matrix\n",
    "                validation_data = np.column_stack([validation_X, validation_y])\n",
    "                training_data = np.column_stack([training_X, training_y])\n",
    "\n",
    "                scalers.append(scaler)\n",
    "\n",
    "                if test_y_end == included_rows3[1]:\n",
    "                    flag = 1\n",
    "\n",
    "                row_number = row_number + 1\n",
    "            else:\n",
    "                if test_y_end > included_rows3[1]:\n",
    "                    flag = 1\n",
    "\n",
    "                row_number = row_number + 1\n",
    "\n",
    "            if validation_data is None or training_data is None:\n",
    "                continue\n",
    "\n",
    "            if np.all(validation_data[:, :-1] == 0.):\n",
    "                continue\n",
    "\n",
    "            win_X = np.array(training_data[:, :-1])\n",
    "            win_y = np.expand_dims(np.array(training_data[:, -1]), 1)\n",
    "            model.fit(win_X, win_y)\n",
    "\n",
    "            # Predict the target values\n",
    "            y_train_pred = model.predict(win_X)\n",
    "            train_mse_scores.append(mean_squared_error(win_y, y_train_pred))\n",
    "            train_q2_scores.append(None)  # TODO\n",
    "            train_r2_scores.append(r2_score(win_y, y_train_pred))\n",
    "\n",
    "            win_X = validation_data[:, :-1]\n",
    "            win_y = np.expand_dims(np.array(validation_data[:, -1]), 1)\n",
    "            # Predict the target values\n",
    "            y_valid_pred = model.predict(win_X)\n",
    "            valid_mse_scores.append(mean_squared_error(win_y, y_valid_pred))\n",
    "            valid_q2_scores.append(None)  # TODO\n",
    "            valid_r2_scores.append(r2_score(win_y, y_valid_pred))\n",
    "\n",
    "        models.append(Model((window_size, num_of_components), model, scalers,\n",
    "                            train_mse_scores, train_q2_scores, train_r2_scores,\n",
    "                            valid_mse_scores, valid_q2_scores, valid_r2_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodels\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Test model\n",
    "#\n",
    "# window_size = 20  # best window_size\n",
    "# num_of_components = 7 # best num_of_components\n",
    "#\n",
    "# test_size = round(0.15*window_size)\n",
    "# test_start = '2017-04-23 00:00:00' # example start of test partition\n",
    "# test_start = datetime.datetime.strptime(test_start, '%Y-%m-%d %H:%M:%S')\n",
    "# flag = 0\n",
    "#\n",
    "# row_number = df.index.get_loc(test_start)\n",
    "#\n",
    "# # Iterate through the data\n",
    "# while flag == 0:\n",
    "#     test_y_end = row_number+test_size+1\n",
    "#     training_X_start = row_number - round(0.85*window_size)\n",
    "#     # Stay within included rows\n",
    "#     if ((test_y_end <= included_rows1[1] and training_X_start >= included_rows1[0])\n",
    "#     or (test_y_end <= included_rows2[1] and training_X_start >= included_rows2[0])\n",
    "#     or (test_y_end <= included_rows3[1] and training_X_start >= included_rows3[0])):\n",
    "#\n",
    "#         scaler = StandardScaler()\n",
    "#\n",
    "#         # Testing set\n",
    "#         test_X = df_array[row_number:test_y_end-1,:-1]\n",
    "#         test_y = df_array[row_number+1:test_y_end,-1]\n",
    "#\n",
    "#         # Training (+ validation) set\n",
    "#         training_X = df_array[training_X_start:row_number,:-1]\n",
    "#         training_y = df_array[training_X_start+1:row_number+1,-1]\n",
    "#\n",
    "#         # Lagged predictor variables and the response variable in one matrix\n",
    "#         test_data = np.column_stack([test_X, test_y])\n",
    "#         training_data = np.column_stack([training_X, training_y])\n",
    "#\n",
    "#         # Compute the mean and std with training data\n",
    "#         scaler.fit(training_data)\n",
    "#         # Perform standardization\n",
    "#         training_data = scaler.transform(training_data)\n",
    "#         test_data = scaler.transform(test_data)\n",
    "#\n",
    "#         if test_y_end == included_rows3[1]:\n",
    "#             flag = 1\n",
    "#\n",
    "#         row_number = row_number + 1\n",
    "#     else:\n",
    "#         if test_y_end > included_rows3[1]:\n",
    "#             flag = 1\n",
    "#\n",
    "#         row_number = row_number + 1\n",
    "#\n",
    "#         # # TODO model fitting\n",
    "#         # for _ in [1]:\n",
    "#         #     ...\n",
    "#\n",
    "#         # # TODO model testing\n",
    "#         # for _ in [1]:\n",
    "#         #     ...\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}