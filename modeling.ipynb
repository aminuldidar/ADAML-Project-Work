{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### -Importing necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Load the dataset and preprocess\n",
    "df = pd.read_csv(\"preprocessed_dataset.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example division of data into training, validation and test sets for the normal functioning times:\n",
    "# data: data from each minute\n",
    "# about 70% for training, 15% for validation, 15% for testing\n",
    "\n",
    "# Reduce the amount of observations (with averaging)\n",
    "df = df.resample(\"T\").mean()\n",
    "\n",
    "included_rows1 = [df.index.get_loc(datetime.datetime(2017,4,23,19,0,0)),\n",
    "                    df.index.get_loc(datetime.datetime(2017,5,13,0,59,00))]\n",
    "included_rows2 = [df.index.get_loc(datetime.datetime(2017,6,15,1,0,0)),\n",
    "                    df.index.get_loc(datetime.datetime(2017,7,24,0,59,00))]\n",
    "included_rows3 = [df.index.get_loc(datetime.datetime(2017,8,15,1,0,0)),\n",
    "                    df.index.get_loc(datetime.datetime(2017,9,10,0,0,0))]\n",
    "\n",
    "test_mins = 5\n",
    "training_n = 5 # training mins = training_n*test_mins\n",
    "test_start = '2017-04-23 00:00:00' # example start of test partition\n",
    "test_start = datetime.datetime.strptime(test_start, '%Y-%m-%d %H:%M:%S')\n",
    "flag = 0\n",
    "\n",
    "df_array = df.to_numpy()\n",
    "row_number = df.index.get_loc(test_start)\n",
    "\n",
    "# Iterate through the data\n",
    "while flag == 0:\n",
    "    test_y_end = row_number+test_mins+1\n",
    "    training_X_start = row_number-(training_n+1)*test_mins\n",
    "    # Stay within included rows\n",
    "    if ((test_y_end <= included_rows1[1] and training_X_start >= included_rows1[0])\n",
    "    or (test_y_end <= included_rows2[1] and training_X_start >= included_rows2[0])\n",
    "    or (test_y_end <= included_rows3[1] and training_X_start >= included_rows3[0])):\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Testing set\n",
    "        test_X = df_array[row_number:test_y_end-1,:-1]\n",
    "        test_y = df_array[row_number+1:test_y_end,-1]\n",
    "\n",
    "        # Validation set\n",
    "        validation_X = df_array[row_number-test_mins:row_number,:-1]\n",
    "        validation_y = df_array[row_number-test_mins+1:row_number+1,-1]\n",
    "\n",
    "        # Training set\n",
    "        training_X = df_array[training_X_start:row_number-test_mins,:-1]\n",
    "        training_y = df_array[training_X_start+1:row_number-test_mins+1,-1]\n",
    "\n",
    "        # Lagged predictor variables and the response variable in one matrix\n",
    "        test_data = np.column_stack([test_X, test_y])\n",
    "        validation_data = np.column_stack([validation_X, validation_y])\n",
    "        training_data = np.column_stack([training_X, training_y])\n",
    "\n",
    "        # Compute the mean and std with training data\n",
    "        scaler.fit(training_data)\n",
    "        # Perform standardization\n",
    "        training_data = scaler.transform(training_data)\n",
    "        validation_data = scaler.transform(validation_data)\n",
    "        test_data = scaler.transform(test_data)\n",
    "\n",
    "        if test_y_end == included_rows3[1]:\n",
    "            flag = 1\n",
    "\n",
    "        row_number = row_number + 1\n",
    "    else:\n",
    "        if test_y_end > included_rows3[1]:\n",
    "            flag = 1\n",
    "\n",
    "        row_number = row_number + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}